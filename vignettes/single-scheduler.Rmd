---
title: "Scheduled RStudio"
author: "Mark Edmondson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Scheduled RStudio}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

See also scheduling via a [master/slave cluster](articles/scheduled-rscripts.html)

# RStudio server + scheduler

This workflow demonstrates how you can take advatage of [Dockerfiles](https://docs.docker.com/engine/reference/builder/) to customise the VM templates.  

Using `Dockerfiles` is recommended if you are making a lot of changes to a template, as its a lot easier to keep track on what is happening.

In summary:

1. Launch a template VM with the container you want to base yours upon
2. Construct a `Dockerfile` in a folder with any other files or dependencies, such as cron
3. Use `docker_build` to upload and build your custom Docker image on the VM
4. Save your custom image to the Container Registry
5. Launch another VM to run your custom image
6. Schedule a script to download from Google Analytics, send an email and upload to BigQuery

### Launch a template VM

Build VMs should be more powerful than the default machine type (`f1-micro`) else there is a danger of it hanging for big expensive builds. 

```r
library(googleComputeEngineR)

## installs rocker/hadleyverse docker image
vm <- gce_vm(name = "build-schedule-r", 
             template = "rstudio-hadleyverse", 
             predefined_type = "n1-standard-2")
```

### Construct a `Dockerfile`

The `Dockerfile` here is available via `get_dockerfile("hadleyverse-crontab")`.  

It is shown below, which you could base your own upon.  This one installs `cron` for scheduling, and `nano` a simple text editor. It then also installs some libraries needed for my scheduled scripts:

From CRAN:

* `googleAuthR` - google authentication
* `shinyFiles` - for cron jobs
* `googleCloudStorageR` - for uploading to Google Cloud Storage
* `bigQueryR` - for uploading to BigQuery
* `gmailR` - an email R package 
* `googleAnalyticsR` - for downloading Google Analytics data

From Github 

* `bnosac/cronR` - to help with creating cron jobs within RStudio. 


```sh
FROM rocker/hadleyverse
MAINTAINER Mark Edmondson (r@sunholo.com)

# install cron and R package dependencies
RUN apt-get update && apt-get install -y \
    cron \
    nano \
    ## clean up
    && apt-get clean \ 
    && rm -rf /var/lib/apt/lists/ \ 
    && rm -rf /tmp/downloaded_packages/ /tmp/*.rds
    
## Install packages from CRAN
RUN install2.r --error \ 
    -r 'http://cran.rstudio.com' \
    googleAuthR shinyFiles googleCloudStorageR bigQueryR gmailr googleAnalyticsR \
    ## install Github packages
    && Rscript -e "devtools::install_github(c('bnosac/cronR'))" \
    ## clean up
    && rm -rf /tmp/downloaded_packages/ /tmp/*.rds \

## Start cron
RUN sudo service cron start
```

### Using `docker_build`

We now upload to the instance the Dockerfile, and build a new docker image called `my-cron-verse`.

This can take some time the first time, so its time for another cup of tea. 

```r
## build a new image based on rocker/hadleyverse with this Dockerfile
docker_build(vm, 
             dockerfile = get_dockerfile("hadleyverse-crontab"), 
             new_image = "my-cron-verse")

## wait for it to build (5mins +)
# ...
# [0m ---> 059fe3ed926a
# Removing intermediate container 7695f3dc071f
# Successfully built 059fe3ed926a
# Using existing public key in /Users/mark/.ssh/google_compute_engine.pub
# REPOSITORY           TAG                 IMAGE ID            CREATED             SIZE
# my-cron-verse        latest              059fe3ed926a        2 seconds ago       1.933 GB
# rocker/hadleyverse   latest              05e3b636e90b        24 hours ago        1.921 GB
```

### Save your custom image to the Container Registry

This image is now saved to the Container Registry

```r
gce_push_registry(vm, save_name = "my-cron-verse", image_name = "myrstudio")
```

Once that is done, we don't need this instance anymore. 

```r
gce_vm_delete(vm)
```

### Launch another VM to run your custom image

You can now launch instances using your constructed image.  

You can also use your custom image to create further `Dockerfiles` that use it as a dependency, using `gce_tag_container()` to get its correct name. 

```r
## now start an instance using our rstudio image in cloud-config
## this takes care of rstudio friendly settings, restart behaviour etc.
tag <- gce_tag_container("my-rstudio")

## rstudio template, but with your private rstudio build
vm2 <- gce_vm(name = "myrstudio2", 
              template = "rstudio", 
              dynamic_image = tag, 
              username = "mark", 
              password = "mark1234")
``` 

You can check when the images are downloaded by using `gce_check_container()`

```r
## check on progress on the container pull
gce_check_container(vm2, "rstudio")
# -- Logs begin at Thu 2016-11-17 14:54:38 UTC, end at Thu 2016-11-17 14:57:38 UTC. --
# Nov 17 14:54:43 myrstudio2 docker[1045]: Unable to find image 'gcr.io/mark-edmondson-gde/my-rstudio:latest' locally
# Nov 17 14:54:47 myrstudio2 docker[1045]: latest: Pulling from mark-edmondson-gde/my-rstudio
# Nov 17 14:54:47 myrstudio2 docker[1045]: a84f66826a7f: Pulling fs layer
# ...
# ...
# Nov 17 14:58:36 myrstudio2 docker[1045]: [cont-init.d] conf: exited 0.
# Nov 17 14:58:36 myrstudio2 docker[1045]: [cont-init.d] done.
# Nov 17 14:58:36 myrstudio2 docker[1045]: [services.d] starting services
# Nov 17 14:58:36 myrstudio2 docker[1045]: [services.d] done.

## your custom rstudio instance is now ready
> vm2
# ==Google Compute Engine Instance==
#
# Name:                myrstudio2
# Created:             2016-11-17 06:54:18
# Machine Type:        f1-micro
# Status:              RUNNING
# Zone:                europe-west1-b
# External IP:         104.199.67.250
# Disks: 
#             deviceName       type       mode boot autoDelete
# 1 myrstudio2-boot-disk PERSISTENT READ_WRITE TRUE       TRUE
```

You can delete your instances, knowing that the custom image is safe in the Container Registry, or just stop them using `gce_vm_stop()` and start again with `gce_vm_start()`

```r
## delete the instance (the container is safe)
gce_vm_delete(vm2)
```

### A demo script

A demo script for scheduling is below.  

It is not recommended to put critical data within a Docker contianer, as it can be destroyed if the container crashes.  Instead, call dedicated data stores such as BigQuery or Cloud Storage, which as you are using Google Compute Engine you already have access to under the same project. 

 In summary the script below:
 
 1. Downloads data from Google Analytics
 2. Uploads the data to BigQuery
 3. Uploads the data to Google Cloud Storage
 3. Sends an email giving the daily total
 
Log into your RStudio Server instance and create the following script:

```r
library(googleCloudStorageR)
library(bigQueryR)
library(gmailr)
library(googleAnalyticsR)

## set options for authentication
options(googleAuthR.client_id = XXXXX)
options(googleAuthR.client_secret = XXXX)
options(googleAuthR.scopes.selected = c("https://www.googleapis.com/auth/cloud-platform",
                                        "https://www.googleapis.com/auth/analytics.readonly"))

## authenticate
## using service account, ensure service account email added to GA account, BigQuery user permissions set, etc.
googleAuthR::gar_auth_service("auth.json")

## get Google Analytics data
gadata <- google_analytics_4(123456, 
                             date_range = c(Sys.Date() - 2, Sys.Date() - 1),
                             metrics = "sessions",
                             dimensions = "medium",
                             anti_sample = TRUE)

## upload to Google BigQuery
bqr_upload_data(projectId = "myprojectId", 
                datasetId = "mydataset",
                tableId = paste0("gadata_",format(Sys.Date(),"%Y%m%d")),
                upload_data = gadata,
                create = TRUE)

## upload to Google Cloud Storage
gcs_upload(gadata, name = paste0("gadata_",Sys.Date(),".csv"))


## get top medium referrer
top_ref <- paste(gadata[order(gadata$sessions, decreasing = TRUE),][1, ], collapse = ",")
# 3456, organic

## send email with todays figures
daily_email <- mime(
  To = "bob@myclient.com",
  From = "bill@cool-agency.com",
  Subject = "Todays winner is....",
  body = paste0("Top referrer was: "),top_ref)
send_message(daily_email)
```

Save the script within RStudio as `daily-report.R`

You can then use [`cronR`](https://github.com/bnosac/cronR) to schedule the script for a daily extract.  

Use `cronR`'s RStudio addin, or in the console issue:

```r
library(cronR)
cron_add(paste0("Rscript ", normalizePath("daily-report")), frequency = "daily")
# Adding cronjob:
# ---------------
#
# ## cronR job
# ## id:   fe9168c7543cc83c1c2489de82216c0f
# ## tags: 
# ## desc: 
# 0 0 * * * Rscript /home/mark/demo-schedule.R
```

The script will then run every day.  

Test the script locally and with a test schedule before using in production.  Once satisfied, you can run locally the `gce_push_registry()` again to save the RStudio image with your scehduled script embedded within. 

If you want to call the scheduled data from a Shiny app, you can now fetch the data again via `bqr_query` from `bigQueryR` or `gcs_get_object` from `googleCloudStorageR` within your `server.R` to pull in the data into your app at runtime. 
